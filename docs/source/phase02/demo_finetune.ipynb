{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Fine-Tuning NM Results Management Language Model with a Custom Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be using a sample of 10 radiology reports to show how we can preprocess the data, load the NM Results Management language model checkpoints, and use them for fine-tuning on your in-house data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the data is loaded. In this example, we will train the model to perform a three-class classification problem, determining whether a report contains lung, adrenal, or no findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rpt_num</th>\n",
       "      <th>note</th>\n",
       "      <th>selected_finding</th>\n",
       "      <th>selected_proc</th>\n",
       "      <th>selected_label</th>\n",
       "      <th>new_note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PROCEDURE:  CT CHEST WO CONTRAST. HISTORY:  Wheezing TECHNIQUE:  Non-contrast helical thoracic CT was performed. COMPARISON:  There is no prior chest CT for comparison. FINDINGS:   Support Devices:  None. Heart/Pericardium/Great Vessels:        Cardiac size is normal.      There is no calcific coronary artery atherosclerosis.       There is no pericardial effusion.      The aorta is normal in diameter.      The main pulmonary artery is normal in diameter. Pleural Spaces:  Few small pleural calcifications are present in the right pleura for example on 2/62 and 3/76.  The pleural spaces are otherwise clear. Mediastinum/Hila:  There is no mediastinal or hilar lymph node enlargement.  Subcentimeter minimally calcified paratracheal lymph nodes are likely related to prior granulomas infection. Neck Base/Chest Wall/Diaphragm/Upper Abdomen:  There is no supraclavicular or axillary lymph node enlargement.  Limited, non-contrast imaging through the upper abdomen is within normal limits.  Mild degenerative change is present in the spine. Lungs/Central Airways: There is a 15 mm nodular density in the nondependent aspect of the bronchus intermedius on 2/52.  The trachea and central airways are otherwise clear.  There is mild diffuse bronchial wall thickening.  There is a calcified granuloma in the posterior right upper lobe.  The lungs are otherwise clear. CONCLUSIONS:   1.  There is mild diffuse bronchial wall thickening suggesting small airways disease such as asthma or bronchitis in the appropriate clinical setting. 2.  A 3 mm nodular soft tissue attenuation in the nondependent aspect of the right bronchus intermedius is nonspecific, which could be mucus or abnormal soft tissue.  A follow-up CT in 6 months might be considered to evaluate the growth. 3.  Stigmata of old granulomatous disease is present. &amp;#x20; FINAL REPORT Attending Radiologist:</td>\n",
       "      <td>Lung Findings</td>\n",
       "      <td>CT Chest</td>\n",
       "      <td>A 3 mm nodular soft tissue attenuation in the nondependent aspect of the right bronchus intermedius is nonspecific, which could be mucus or abnormal soft tissue.  A follow-up CT in 6 months might be considered to evaluate the growth.</td>\n",
       "      <td>support devices:  none. heart/pericardium/great vessels:        cardiac size is normal.      there is no calcific coronary artery atherosclerosis.       there is no pericardial effusion.      the aorta is normal in diameter.      the main pulmonary artery is normal in diameter. pleural spaces:  few small pleural calcifications are present in the right pleura for example on 2/62 and 3/76.  the pleural spaces are otherwise clear. mediastinum/hila:  there is no mediastinal or hilar lymph node enlargement.  subcentimeter minimally calcified paratracheal lymph nodes are likely related to prior granulomas infection. neck base/chest wall/diaphragm/upper abdomen:  there is no supraclavicular or axillary lymph node enlargement.  limited, non-contrast imaging through the upper abdomen is within normal limits.  mild degenerative change is present in the spine. lungs/central airways: there is a 15 mm nodular density in the nondependent aspect of the bronchus intermedius on 2/52.  the trachea and central airways are otherwise clear.  there is mild diffuse bronchial wall thickening.  there is a calcified granuloma in the posterior right upper lobe.  the lungs are otherwise clear. conclusions:   1.  there is mild diffuse bronchial wall thickening suggesting small airways disease such as asthma or bronchitis in the appropriate clinical setting. 2.  a 3 mm nodular soft tissue attenuation in the nondependent aspect of the right bronchus intermedius is nonspecific, which could be mucus or abnormal soft tissue.  a follow-up ct in 6 months might be considered to evaluate the growth. 3.  stigmata of old granulomatous disease is present.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>PROCEDURE:  CT ABDOMEN PELVIS W CONTRAST COMPARISON:  date INDICATIONS:  Lower abdominal/flank pain on the right TECHNIQUE:     After obtaining the patients consent, CT images were created with intravenous iodinated contrast.  FINDINGS:   LIVER:   The liver is normal in size.  No suspicious liver lesion is seen. The portal and hepatic veins are patent. BILIARY:   No biliary duct dilation. The biliary system is otherwise unremarkable. PANCREAS:   No focal pancreatic lesion.  No pancreatic duct dilation. SPLEEN:   No suspicious splenic lesion is seen. The spleen is normal in size. KIDNEYS:   No suspicious renal lesion is seen.  No hydronephrosis. ADRENALS:   No adrenal gland nodule or thickening.  AORTA/VASCULAR:   No aneurysm. RETROPERITONEUM:   No lymphadenopathy. BOWEL/MESENTERY:   The appendix is normal.  No bowel wall thickening or bowel dilation. ABDOMINAL WALL:   No hernia. URINARY BLADDER:   Incomplete bladder distension limits evaluation, but no focal wall thickening or calculus is seen. PELVIC NODES:   No lymphadenopathy.  PELVIC ORGANS:   Status post hysterectomy.  No pelvic mass. BONES:   No acute fracture or suspicious osseous lesion. LUNG BASES:   No pleural effusion or consolidation. OTHER:   Small hiatal hernia. CONCLUSION:   1.  No acute process is detected. 2.  Small hiatal hernia &amp;#x20; FINAL REPORT Attending Radiologist:</td>\n",
       "      <td>No Findings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No label</td>\n",
       "      <td>liver:   the liver is normal in size.  no suspicious liver lesion is seen. the portal and hepatic veins are patent. biliary:   no biliary duct dilation. the biliary system is otherwise unremarkable. pancreas:   no focal pancreatic lesion.  no pancreatic duct dilation. spleen:   no suspicious splenic lesion is seen. the spleen is normal in size. kidneys:   no suspicious renal lesion is seen.  no hydronephrosis. adrenals:   no adrenal gland nodule or thickening.  aorta/vascular:   no aneurysm. retroperitoneum:   no lymphadenopathy. bowel/mesentery:   the appendix is normal.  no bowel wall thickening or bowel dilation. abdominal wall:   no hernia. urinary bladder:   incomplete bladder distension limits evaluation, but no focal wall thickening or calculus is seen. pelvic nodes:   no lymphadenopathy.  pelvic organs:   status post hysterectomy.  no pelvic mass. bones:   no acute fracture or suspicious osseous lesion. lung bases:   no pleural effusion or consolidation. other:   small hiatal hernia. conclusion:   1.  no acute process is detected. 2.  small hiatal hernia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>EXAM:  MRI ABDOMEN W WO CONTRAST CLINICAL INDICATION:  Cirrhosis of liver without ascites, unspecified hepatic cirrhosis type (CMS-HCC) TECHNIQUE: MRI of the abdomen was performed with and without contrast. Multiplanar imaging was performed.  8.5  cc of Gadavist was administered. COMPARISON:  DATE and priors FINDINGS:   On limited views of the lung bases, no acute abnormality is noted. There may be mild distal esophageal wall thickening. On the out of phase series, there is suggestion of some signal gain within the hepatic parenchyma. This is stable. A tiny cystic nonenhancing focus is seen anteriorly in the right hepatic lobe (9/10), unchanged. A subtly micronodular hepatic periphery is noted. There are few subtle hypervascular lesions in the right hepatic lobe, without significant washout. The portal vein is patent. Some splenorenal shunting is redemonstrated, similar to the comparison exam. The spleen measures 12.4 cm in length. No focal splenic lesion is appreciated. There are several small renal lesions again seen, many of which again demonstrate T1 shortening. On the postcontrast subtraction series, no obvious enhancement is noted. The adrenal glands and pancreas are intact. There is mild cholelithiasis, without gallbladder wall thickening or pericholecystic fluid. No free abdominal fluid is visualized. IMPRESSION:   1. Stable cirrhotic appearance of the liver. Few subtly hypervascular hepatic lesions do not demonstrate washout, and probably relate to perfusion variants. No particularly suspicious hepatic mass is seen. 2. Mild splenomegaly to 12.4 cm redemonstrated. Splenorenal shunting is again seen. 3. Scattered simple and complex renal cystic lesions, nonenhancing, stable from March 2040. 4. Incidentally, there is evidence of signal gain in the liver on the out of phase series. This occasionally may represent iron overload.  &amp;#x20; FINAL REPORT Attending Radiologist:</td>\n",
       "      <td>No Findings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No label</td>\n",
       "      <td>on limited views of the lung bases, no acute abnormality is noted. there may be mild distal esophageal wall thickening. on the out of phase series, there is suggestion of some signal gain within the hepatic parenchyma. this is stable. a tiny cystic nonenhancing focus is seen anteriorly in the right hepatic lobe (9/10), unchanged. a subtly micronodular hepatic periphery is noted. there are few subtle hypervascular lesions in the right hepatic lobe, without significant washout. the portal vein is patent. some splenorenal shunting is redemonstrated, similar to the comparison exam. the spleen measures 12.4 cm in length. no focal splenic lesion is appreciated. there are several small renal lesions again seen, many of which again demonstrate t1 shortening. on the postcontrast subtraction series, no obvious enhancement is noted. the adrenal glands and pancreas are intact. there is mild cholelithiasis, without gallbladder wall thickening or pericholecystic fluid. no free abdominal fluid is visualized. impression:   1. stable cirrhotic appearance of the liver. few subtly hypervascular hepatic lesions do not demonstrate washout, and probably relate to perfusion variants. no particularly suspicious hepatic mass is seen. 2. mild splenomegaly to 12.4 cm redemonstrated. splenorenal shunting is again seen. 3. scattered simple and complex renal cystic lesions, nonenhancing, stable from march 2040. 4. incidentally, there is evidence of signal gain in the liver on the out of phase series. this occasionally may represent iron overload.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Define the path to the data\n",
    "base_path = os.path.dirname(\"__file__\")\n",
    "data_path = os.path.abspath(os.path.join(base_path, \"..\", \"demo_data.gz\"))\n",
    "\n",
    "# Import data\n",
    "modeling_df = joblib.load(data_path)\n",
    "\n",
    "display(HTML(modeling_df.head(3).to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the impression (i.e., the findings / conclusions section) of the report is extracted, any doctor signatures are removed, and the report lowercased. This preprocessing section may need to be modified to accommodate your healthcare system's reports, formatting, etc. The ``preprocess_note`` function is modified from ``nmrezman.utils.preprocess_input``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_split(x, keywords, return_idx: int=2):\n",
    "    \"\"\"\n",
    "    Extract portion of string given a list of possible delimiters (keywords) via partition method\n",
    "    \"\"\"\n",
    "    for keyword in keywords:\n",
    "        if x.partition(keyword)[2] !='':\n",
    "            return x.partition(keyword)[return_idx]\n",
    "    return x\n",
    "    \n",
    "def preprocess_note(note):\n",
    "    \"\"\"\n",
    "    Get the impression from the note, remove doctor signature, and lowercase\n",
    "    \"\"\"\n",
    "    impression_keywords = [\n",
    "            \"impression:\",\n",
    "            \"conclusion(s):\",\n",
    "            \"conclusions:\",\n",
    "            \"conclusion:\",\n",
    "            \"finding:\",\n",
    "            \"findings:\",\n",
    "    ]\n",
    "    signature_keywords = [\n",
    "        \"&#x20\",\n",
    "        \"final report attending radiologist:\",\n",
    "    ]\n",
    "    impressions = keyword_split(str(note).lower(), impression_keywords)\n",
    "    impressions = keyword_split(note, signature_keywords, return_idx=0)\n",
    "    return impressions\n",
    "\n",
    "# Preprocess the note\n",
    "modeling_df[\"impression\"] = modeling_df[\"note\"].apply(preprocess_note)\n",
    "modeling_df = modeling_df[modeling_df[\"impression\"].notnull()]\n",
    "modeling_df[\"impression\"] = modeling_df[\"impression\"].apply(lambda x: str(x.encode('utf-8')) +\"\\n\"+\"\\n\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we encode the findings label into integer labels for the model to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Encode the Lung, Adrenal, and No Finding into integer labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(modeling_df[\"selected_finding\"])\n",
    "modeling_df[\"int_labels\"] = le.transform(modeling_df[\"selected_finding\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is split into train and test sets (as lists so that it is formatted for the ``Dataset``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test\n",
    "train_df, test_df = train_test_split(modeling_df, test_size=0.3, stratify=modeling_df[\"selected_finding\"], random_state=37)\n",
    "train_note = list(train_df[\"impression\"])\n",
    "train_label = list(train_df[\"int_labels\"])\n",
    "test_note = list(test_df[\"impression\"])\n",
    "test_label = list(test_df[\"int_labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Define the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a tokenizer to mask words or word fragments to tokens. Here, we are using [ðŸ¤—'s pretrained RoBERTa base model's](https://huggingface.co/roberta-base) checkpoint. Padding is done on the left side since NM radiology reports generally have the findings at the end of the report. Note that you can change out the tokenizer and model to start from a different RoBERTa checkpoint (e.g., ``roberta-large``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Define the tokenizer (from a pre-trained checkpoint) and tokenize the notes\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", use_fast=True, padding_side=\"left\")\n",
    "train_encodings = tokenizer(train_note, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(test_note, truncation=True, padding=True)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a custom Pytorch ``Dataset`` class. This will return the tokenized report text and integer label for a given index. ðŸ¤— can easily use custom Pytorch ``Dataset``s for training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Reports_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings: dict, labels: list) -> None:\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        return\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "    \n",
    "# Define the trainign dataset with tokenized notes and labels\n",
    "train_dataset = Reports_Dataset(train_encodings, train_label)\n",
    "val_dataset = Reports_Dataset(val_encodings, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the pretrained model (similar to the one that was pretrained via the notebook ``Demo: Phase 02 Pretraining the NM Results Management Language Model with Custom Corpus`` only trained on thousands of reports). This model will be fine-tuned to a specific task, which, in this case, is a multi-class classification problem that determines if a report has Lung Findings, Adrenal Findings, or No Findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /path/to/results/phase02/demo/checkpoint-14500 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /path/to/results/phase02/demo/checkpoint-14500 and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# TODO: point to the pretrained model trained as part of the pretraining process\n",
    "# Here, we are using a pretrained checkpoint trained on thousands of reports (vs the pretrained model wieghts generated via the notebook ``demo_pretrain``)\n",
    "# To use the only directly trained by the notebook, use \"/path/to/results/phase02/demo/checkpoint-4\" \n",
    "model_pretrained_path = \"/path/to/results/phase02/demo/checkpoint-14500\"\n",
    "\n",
    "# Fine-tune the model from the pre-trained checkpoint\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_pretrained_path, num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we begin training using the ðŸ¤— ``Trainer``, which will train according to the parameters specified in the ðŸ¤— ``TrainingArguments``. ðŸ¤— will take care of all the training for us! When done, the last checkpoint will be used as for classifying reports for Lung, Adrenal, or No Findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 7\n",
      "  Num Epochs = 40\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 40\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 01:15, Epoch 40/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.090820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.093700</td>\n",
       "      <td>1.090332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.093700</td>\n",
       "      <td>1.089355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.086800</td>\n",
       "      <td>1.088135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.086800</td>\n",
       "      <td>1.086182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.101900</td>\n",
       "      <td>1.083984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.101900</td>\n",
       "      <td>1.081299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.077500</td>\n",
       "      <td>1.078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.077500</td>\n",
       "      <td>1.074707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.096700</td>\n",
       "      <td>1.071045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.096700</td>\n",
       "      <td>1.066895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.069300</td>\n",
       "      <td>1.063110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.069300</td>\n",
       "      <td>1.058838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.042600</td>\n",
       "      <td>1.054077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.042600</td>\n",
       "      <td>1.049561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.043200</td>\n",
       "      <td>1.044678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.043200</td>\n",
       "      <td>1.039795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.978700</td>\n",
       "      <td>1.034790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.978700</td>\n",
       "      <td>1.030029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.941100</td>\n",
       "      <td>1.025269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.941100</td>\n",
       "      <td>1.020630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.910500</td>\n",
       "      <td>1.016968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.910500</td>\n",
       "      <td>1.013428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.869600</td>\n",
       "      <td>1.010620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.869600</td>\n",
       "      <td>1.007324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.771500</td>\n",
       "      <td>1.003967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.771500</td>\n",
       "      <td>0.999207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.772400</td>\n",
       "      <td>0.993286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.772400</td>\n",
       "      <td>0.985840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.708900</td>\n",
       "      <td>0.975403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.708900</td>\n",
       "      <td>0.966370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.651300</td>\n",
       "      <td>0.960327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.651300</td>\n",
       "      <td>0.958374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.524000</td>\n",
       "      <td>0.957214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.524000</td>\n",
       "      <td>0.954956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.412900</td>\n",
       "      <td>0.951202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.412900</td>\n",
       "      <td>0.948364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>0.949860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>0.955627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.311500</td>\n",
       "      <td>0.955841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-1\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-1/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-1/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-14] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-2\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-2/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-2/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-15] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-3\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-3/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-3/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-1] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-4\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-4/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-4/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-2] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-5\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-5/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-5/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-3] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-6\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-6/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-6/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-4] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-7\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-7/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-7/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-5] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-8\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-8/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-8/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-6] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-9\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-9/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-9/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-7] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-10\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-10/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-10/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-8] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-11\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-11/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-11/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-9] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-12\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-12/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-12/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-13\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-13/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-13/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-11] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-14\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-14/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-14/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-12] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-15\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-15/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-15/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-13] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-16\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-16/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-16/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-14] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-17\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-17/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-17/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-15] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-18\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-18/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-18/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-16] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-19\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-19/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-19/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-17] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-20\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-20/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-20/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-18] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-21\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-21/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-21/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-19] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-22\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-22/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-22/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-23\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-23/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-23/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-21] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-24\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-24/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-24/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-22] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-25\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-25/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-25/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-23] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-26\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-26/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-26/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-24] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-27\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-27/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-27/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-25] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-28\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-28/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-28/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-26] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-29\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-29/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-29/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-27] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-30\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-30/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-30/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-28] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-31\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-31/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-31/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-29] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-32\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-32/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-33\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-33/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-33/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-31] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-34\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-34/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-34/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-35\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-35/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-35/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-33] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-36\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-36/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-36/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-34] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-37\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-37/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-37/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-35] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-38\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-38/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-38/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-36] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-39\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-39/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-39/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-38] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /path/to/results/phase02/demo/findings/checkpoint-40\n",
      "Configuration saved in /path/to/results/phase02/demo/findings/checkpoint-40/config.json\n",
      "Model weights saved in /path/to/results/phase02/demo/findings/checkpoint-40/pytorch_model.bin\n",
      "Deleting older checkpoint [/path/to/results/phase02/demo/findings/checkpoint-39] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /path/to/results/phase02/demo/findings/checkpoint-37 (score: 0.9483642578125).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40, training_loss=0.8405103325843811, metrics={'train_runtime': 75.1838, 'train_samples_per_second': 3.724, 'train_steps_per_second': 0.532, 'total_flos': 37091533086720.0, 'train_loss': 0.8405103325843811, 'epoch': 40.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define the training parameters and ðŸ¤— Trainer\n",
    "training_args = TrainingArguments(\n",
    "                    output_dir=\"/path/to/results/phase02/demo/findings\",    # output directory\n",
    "                    num_train_epochs=40,                                    # total number of training epochs\n",
    "                    per_device_train_batch_size=16,                         # batch size per device during training\n",
    "                    per_device_eval_batch_size=8,                           # batch size per device during evaluation\n",
    "                    warmup_steps=100,                                       # number of warmup steps for learning rate scheduler\n",
    "                    weight_decay=0.015,                                     # strength of weight decay\n",
    "                    fp16=True,                                              # mixed precision training\n",
    "                    do_predict=True,                                        # run predictions on test set\n",
    "                    load_best_model_at_end=True,                            # load best model at end so we can run confusion matrix\n",
    "                    logging_steps=2,                                        # remaining args are related to logging\n",
    "                    save_total_limit=2,\n",
    "                    evaluation_strategy=\"epoch\",\n",
    "                    save_strategy=\"epoch\",              \n",
    "                    report_to=\"none\",                  \n",
    ")\n",
    "trainer = Trainer(\n",
    "                    model=model,                                            # the instantiated ðŸ¤— Transformers model to be trained\n",
    "                    args=training_args,                                     # training arguments, defined above\n",
    "                    train_dataset=train_dataset,                            # training dataset\n",
    "                    eval_dataset=val_dataset,                               # test (evaluation) dataset: save and eval strategy to match\n",
    ")\n",
    "\n",
    "# Train!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `sklearn`'s ``classification_report`` and ``confusion_matrix``, we can evaluate how well the model performs on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       0.50      1.00      0.67         1\n",
      "           2       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.75         4\n",
      "   macro avg       0.83      0.83      0.78         4\n",
      "weighted avg       0.88      0.75      0.75         4\n",
      "\n",
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Perform confusion matrix and print the results\n",
    "y_pred = trainer.predict(val_dataset)\n",
    "y_pred = np.argmax(y_pred.predictions, axis=1)\n",
    "report = classification_report(test_label, y_pred)\n",
    "matrix = confusion_matrix(test_label, y_pred)\n",
    "print(report)\n",
    "print(matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}